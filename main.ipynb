{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To make things simpler, the datasets will be renamed: <br>\n",
    "alik05_forest-fire-dataset = dataset1 <br> <br>\n",
    "elmadafri_the-wildfire-dataset = dataset2 <br> <br>\n",
    "kutaykutlu_forest-fire = dataset3 <br> <br>\n",
    "mohnishsaiprasad_forest-fire-images = dataset4 <br> <br>\n",
    "phylake1337_fire-dataset = dataset5 <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_dataset_folder = \"C:/Users/RoscaMitrut/Desktop/Forest-Fire-Detection/datasets\"\n",
    "path_to_dataset_folder = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_path = path_to_dataset_folder + \"datasets/alik05_forest-fire-dataset/Forest Fire Dataset/\"\n",
    "dataset2_path = path_to_dataset_folder + \"datasets/elmadafri_the-wildfire-dataset/the_wildfire_dataset_2n_version/\"\n",
    "dataset3_path = path_to_dataset_folder + \"datasets/kutaykutlu_forest-fire/\"\n",
    "dataset4_path = path_to_dataset_folder + \"datasets/mohnishsaiprasad_forest-fire-images/Data/\"\n",
    "dataset5_path = path_to_dataset_folder + \"datasets/phylake1337_fire-dataset/fire_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_paths(dataset_path,folders_labels):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    for folder,label in folders_labels:\n",
    "        try:\n",
    "            for f in os.scandir(dataset_path+folder):\n",
    "                if f.is_file() and f.name.endswith((\".jpg\", \".png\")):\n",
    "                    dataset_x.append(dataset_path+folder+\"/\"+f.name)\n",
    "                    dataset_y.append(label)\n",
    "        except:\n",
    "            break                \n",
    "    return dataset_x,dataset_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_X, dataset1_Y = load_data_paths(dataset1_path,[(\"Training/fire\",1),(\"Training/nofire\",0),(\"Testing/fire\",1),(\"Testing/nofire\",0)])\n",
    "\n",
    "len(dataset1_X),len(dataset1_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2_X, dataset2_Y = load_data_paths(dataset2_path,[(\"test/fire\",1),(\"test/nofire\",0),(\"train/fire\",1),(\"train/nofire\",0),(\"val/fire\",1),(\"val/nofire\",0)])\n",
    "\n",
    "len(dataset2_X),len(dataset2_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3_X, dataset3_Y = load_data_paths(dataset3_path,[(\"train_fire\",1)])\n",
    "\n",
    "len(dataset3_X),len(dataset3_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset4_X, dataset4_Y = load_data_paths(dataset4_path,[(\"Test_Data/Fire\",1),(\"Test_Data/Non_Fire\",0),(\"Train_Data/Fire\",1),(\"Train_Data/Non_Fire\",0)])\n",
    "\n",
    "len(dataset4_X),len(dataset4_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset5_X, dataset5_Y = load_data_paths(dataset5_path,[(\"fire_images\",1),(\"non_fire_images\",0)])\n",
    "\n",
    "len(dataset5_X),len(dataset5_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting and merging datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_X = np.array(dataset1_X + dataset2_X + dataset3_X + dataset4_X + dataset5_X)\n",
    "dataset_Y = np.array(dataset1_Y + dataset2_Y + dataset3_Y + dataset4_Y + dataset5_Y)\n",
    "\n",
    "def shuffle_data(X, Y):\n",
    "    indices = np.random.permutation(len(X))\n",
    "    return X[indices], Y[indices]\n",
    "\n",
    "dataset_X,dataset_Y = shuffle_data(dataset_X,dataset_Y)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(dataset_X,dataset_Y,test_size=0.16)\n",
    "X_train,X_val,Y_train,Y_val = train_test_split(X_train,Y_train,test_size=0.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset1_X,dataset1_Y,dataset2_X,dataset2_Y,dataset3_X,dataset3_Y,dataset4_X,dataset4_Y,dataset5_X,dataset5_Y,dataset_X,dataset_Y\n",
    "del dataset1_path,dataset2_path,dataset3_path,dataset4_path,dataset5_path,path_to_dataset_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_name, target_size = (256, 256)):\n",
    "    raw = tf.io.read_file(file_name)\n",
    "    tensor = tf.io.decode_image(raw, channels=3, expand_animations=False)\n",
    "    tensor = tf.image.resize(tensor, target_size)\n",
    "    tensor = tf.cast(tensor, tf.float32) / 255.0\n",
    "    return tensor\n",
    "\n",
    "def create_dataset(file_paths, labels, batch_size=32, target_size=(256, 256)):\n",
    "    file_paths_tensor = tf.constant(file_paths)\n",
    "    labels_tensor = tf.constant(labels, dtype=tf.float32)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths_tensor, labels_tensor))\n",
    "    \n",
    "    def _load_and_preprocess(path, label):\n",
    "        image = load_image(path, target_size)\n",
    "        return image, label\n",
    "    \n",
    "    dataset = dataset.map(_load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(X_train, Y_train, batch_size=32, target_size=(256, 256))\n",
    "test_dataset = create_dataset(X_test, Y_test, batch_size=32, target_size=(256, 256))\n",
    "val_dataset = create_dataset(X_val, Y_val, batch_size=32, target_size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataset.take(1):\n",
    "    print(\"Dataset element type:\", type(data))\n",
    "    print(\"Number of elements:\", len(data))\n",
    "    if len(data) == 2:\n",
    "        images, labels = data\n",
    "        print(\"Images shape:\", images.shape)\n",
    "        print(\"Labels shape:\", labels.shape)\n",
    "        print(\"Sample labels:\", labels.numpy())\n",
    "        \n",
    "del data,images,labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=7, validation_data=val_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['precision'], label = 'precision')\n",
    "plt.plot(history.history['recall'], label = 'recall')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history.history['val_precision'], label = 'val_precision')\n",
    "plt.plot(history.history['val_recall'], label = 'val_recall')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "y_pred = model.predict(test_dataset).flatten()\n",
    "y_pred = np.round(y_pred)\n",
    "plot_confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = models.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.15),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.15),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.30),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs_model'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'], label='loss')\n",
    "plt.plot(history2.history['accuracy'], label='accuracy')\n",
    "plt.plot(history2.history['precision'], label = 'precision')\n",
    "plt.plot(history2.history['recall'], label = 'recall')\n",
    "plt.plot(history2.history['val_loss'], label = 'val_loss')\n",
    "plt.plot(history2.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.plot(history2.history['val_precision'], label = 'val_precision')\n",
    "plt.plot(history2.history['val_recall'], label = 'val_recall')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "y_pred = model2.predict(test_dataset).flatten()\n",
    "y_pred = np.round(y_pred)\n",
    "plot_confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fire_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"fire_detection_model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = models.load_model(\"fire_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lloaded_model = models.load_model(\"fire_detection_model2.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "second_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
